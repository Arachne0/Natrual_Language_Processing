{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6aiac1uwbk6JEg+5lHWZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arachne0/Natrual_Language_Processing/blob/master/Natural_Language_Processing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bO2b4TnujzJ",
        "outputId": "ec58518d-b56d-4446-cee8-1658f1703645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://Arachne0: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@github.com/Arachne0/Natrual_Language_Processing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ3nDYm9v9u2",
        "outputId": "3d827d0e-4493-4d8b-c2cd-f7a26194bb93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Natrual_Language_Processing'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 613 bytes | 22.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Natrual_Language_Processing/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1mMAfwg2Cwn",
        "outputId": "ff33c929-5e0e-4a17-c738-643921373d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Commit_NLP_Folder/Natrual_Language_Processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email '1022lsh@naver.com'\n",
        "!git config --global user.name 'Lee_SeungHee'"
      ],
      "metadata": {
        "id": "gwLNoOGd2I0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add --all"
      ],
      "metadata": {
        "id": "oDxql8OF8PcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m 'hi'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OKf2XFa8iW6",
        "outputId": "64624c68-bf64-4ef8-9855-c4b63a165cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzx1V_qS9R1f",
        "outputId": "18426b38-42f4-4c4a-c4cf-5890e75b93ca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ClassificationTrainArguments:\n",
        "\n",
        "    pretrained_model_name: str = field(\n",
        "        default=\"beomi/kcbert-base\",\n",
        "        metadata={\"help\": \"pretrained model name\"}\n",
        "    )\n",
        "    downstream_task_name: str = field(\n",
        "        default=\"document-classification\",\n",
        "        metadata={\"help\": \"The name of the downstream data.\"}\n",
        "    )\n",
        "    downstream_corpus_name: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the downstream data.\"}\n",
        "    )\n",
        "    downstream_corpus_root_dir: str = field(\n",
        "        default=\"/content/Korpora\",\n",
        "        metadata={\"help\": \"The root directory of the downstream data.\"}\n",
        "    )\n",
        "    downstream_model_dir: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The output model dir.\"}\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                    \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        }\n",
        "    )\n",
        "    save_top_k: int = field(\n",
        "        default=1,\n",
        "        metadata={\"help\": \"save top k model checkpoints.\"}\n",
        "    )\n",
        "    monitor: str = field(\n",
        "        default=\"min val_loss\",\n",
        "        metadata={\"help\": \"monitor condition (save top k)\"}\n",
        "    )\n",
        "    seed: int = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"random seed.\"}\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    force_download: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"force to download downstream data and pretrained models.\"}\n",
        "    )\n",
        "    test_mode: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Test Mode enables `fast_dev_run`\"}\n",
        "    )\n",
        "    learning_rate: float = field(\n",
        "        default=5e-5,\n",
        "        metadata={\"help\": \"learning rate\"}\n",
        "    )\n",
        "    epochs: int = field(\n",
        "        default=3,\n",
        "        metadata={\"help\": \"max epochs\"}\n",
        "    )\n",
        "    batch_size: int = field(\n",
        "        default=32,\n",
        "        metadata={\"help\": \"batch size. if 0, Let PyTorch Lightening find the best batch size\"}\n",
        "    )\n",
        "    cpu_workers: int = field(\n",
        "        default=os.cpu_count(),\n",
        "        metadata={\"help\": \"number of CPU workers\"}\n",
        "    )\n",
        "    fp16: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Enable train on FP16\"}\n",
        "    )\n",
        "    tpu_cores: int = field(\n",
        "        default=0,\n",
        "        metadata={\"help\": \"Enable TPU with 1 core or 8 cores\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ClassificationDeployArguments:\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            pretrained_model_name=None,\n",
        "            downstream_model_dir=None,\n",
        "            downstream_model_checkpoint_fpath=None,\n",
        "            max_seq_length=128,\n",
        "    ):\n",
        "        self.pretrained_model_name = pretrained_model_name\n",
        "        self.max_seq_length = max_seq_length\n",
        "        if downstream_model_checkpoint_fpath is not None:\n",
        "            self.downstream_model_checkpoint_fpath = downstream_model_checkpoint_fpath\n",
        "        elif downstream_model_dir is not None:\n",
        "            ckpt_file_names = glob(os.path.join(downstream_model_dir, \"*.ckpt\"))\n",
        "            ckpt_file_names = [el for el in ckpt_file_names if \"temp\" not in el and \"tmp\" not in el]\n",
        "            if len(ckpt_file_names) == 0:\n",
        "                raise Exception(f\"downstream_model_dir \\\"{downstream_model_dir}\\\" is not valid\")\n",
        "            selected_fname = ckpt_file_names[-1]\n",
        "            min_val_loss = os.path.split(selected_fname)[-1].replace(\".ckpt\", \"\").split(\"=\")[-1].split(\"-\")[0]\n",
        "            try:\n",
        "                for ckpt_file_name in ckpt_file_names:\n",
        "                    val_loss = os.path.split(ckpt_file_name)[-1].replace(\".ckpt\", \"\").split(\"=\")[-1].split(\"-\")[0]\n",
        "                    if float(val_loss) < float(min_val_loss):\n",
        "                        selected_fname = ckpt_file_name\n",
        "                        min_val_loss = val_loss\n",
        "            except:\n",
        "                raise Exception(f\"the ckpt file name of downstream_model_directory \\\"{downstream_model_dir}\\\" is not valid\")\n",
        "            self.downstream_model_checkpoint_fpath = selected_fname\n",
        "        else:\n",
        "            raise Exception(\"Either downstream_model_dir or downstream_model_checkpoint_fpath must be entered.\")\n",
        "        print(f\"downstream_model_checkpoint_fpath: {self.downstream_model_checkpoint_fpath}\")"
      ],
      "metadata": {
        "id": "Lrkib8rM_sHV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "PdZh8-wfspDA",
        "outputId": "0c049e06-50e6-486c-ac8b-ddfd9a8369b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7557e38b9af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mratsnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlpbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationTrainArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m args = ClassificationTrainArguments(\n\u001b[1;32m      3\u001b[0m     \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beomi/kcbert_base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdownstream_corpus_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nsmc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdownstream_corpus_root_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/Korpora\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ratsnlp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
        "args = ClassificationTrainArguments(\n",
        "    pretrained_model_name=\"beomi/kcbert_base\",\n",
        "    downstream_corpus_name=\"nsmc\",\n",
        "    downstream_corpus_root_dir=\"/content/Korpora\",\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-doccls\",\n",
        "    learning_rate=5e-5,\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwvDy_yfujRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}