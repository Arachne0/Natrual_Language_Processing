{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWf7phc2wBguWl63CeKlQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arachne0/Natrual_Language_Processing/blob/master/Building_A_Vocabulary_Set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratsnlp"
      ],
      "metadata": {
        "id": "AeHgLoTFlN4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive',force_remount=True)"
      ],
      "metadata": {
        "id": "3UeCBaCdxrQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 말뭉치 내려받기 및 전처리\n",
        "# 오픈소스 파이썬 패키지 코포라를 활용해 BPE 수행대상 말뭉치를 내려받고 전처리한다.\n",
        "from Korpora import Korpora \n",
        "nsmc = Korpora.load(\"nsmc\",force_download=True)"
      ],
      "metadata": {
        "id": "xPAxYdfhx6jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NSMC에 포함된 영화 리뷰들을 순수 텍스트 형태로 코랩 환경의 로컬의 지정된 디렉터리에 저장한다.\n",
        "import os\n",
        "def write_lines(path, lines):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(f'{line}\\n')\n",
        "\n",
        "write_lines(\"/content/train.txt\", nsmc.train.get_all_texts())\n",
        "write_lines(\"/content/test.txt\", nsmc.test.get_all_texts())"
      ],
      "metadata": {
        "id": "kXmlmOYqyPOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 토크나이저 구축\n",
        "# 유니코드 바이트 수준으로 어휘찝합을 구축하고 토큰화를 수행한다.\n",
        "# 한글을 한 글자가 3개의 유니코드 바이트로 표현된다.\n",
        "\"디렉터리 만들기\"\n",
        "import os \n",
        "os.makedirs(\"gdrive/My Drive/nlpbook/bbpe\",exist_ok=True)"
      ],
      "metadata": {
        "id": "hVtVOh69yaEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 바이트 수준 BPE 어휘 집합 구축\n",
        "# 코드 수행이 끝나면 /gdrive/My Drive/nlpbook/bbpe 디렉터리에 vocab.json과 merges.txt가 생성된다.\n",
        "# vocab 은 바이트 수준 BPE의 어휘 집합이며, merges는 바이그램 쌍의 병합 우선 순위이다.\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(\n",
        "    files=[\"/content/train.txt\", \"/content/test.txt\"],\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\"[PAD]\"]\n",
        ")\n",
        "bytebpe_tokenizer.save_model(\"/gdrive/My Drive/nlpbook/bbpe\")"
      ],
      "metadata": {
        "id": "aIyRNNx2AvF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 토크나이저 구축\n",
        "import os\n",
        "os.makedirs(\"/gdrive/My Drive/nlpbook/wordpiece\", exist_ok=True)"
      ],
      "metadata": {
        "id": "yUaJr3ZxK7V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 코드 수행이 끝나면 자신의 구글 드라이브 경로(/gdrive/My Drive/nlpbook/wordpiece)에 vocab.txt가 생성된다.\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files=[\"/content/train.txt\", \"/content/test.txt\"],\n",
        "    vocab_size=10000,\n",
        ")\n",
        "wordpiece_tokenizer.save_model(\"/gdrive/My Drive/nlpbook/wordpiece\")"
      ],
      "metadata": {
        "id": "BuIB-4NFL2fV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
